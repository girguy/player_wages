{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAGUE_PREFIX = 'https://sofifa.com/league/'\n",
    "\n",
    "TEAM_PREFIX = 'https://sofifa.com'\n",
    "\n",
    "LEAGUE_NUMBERS = {\n",
    "    'Premier League': '13',\n",
    "    'Championship': '14',\n",
    "    'La Liga': '53',\n",
    "    'La Liga 2': '54',\n",
    "    'Bundesliga': '19',\n",
    "    '2. Bundesliga': '20',\n",
    "    'Serie A': '31',\n",
    "    'Serie B': '32',\n",
    "    'Ligue 1': '16',\n",
    "    'Ligue 2': '17',\n",
    "    'Eredivisie': '10',\n",
    "    'Pro League': '4',\n",
    "    'SÃ¼per Lig': '68',\n",
    "    'Primeira Liga': '308',    \n",
    "}\n",
    "\n",
    "PLAYER_QUALITIES = [\n",
    "    'Crossing', 'Finishing', 'Heading accuracy', 'Short passing', 'Volleys', 'Dribbling',\n",
    "    'Curve', 'FK Accuracy', 'Long passing', 'Ball control', 'Acceleration', 'Sprint speed',\n",
    "    'Agility', 'Reactions', 'Balance', 'Shot power', 'Jumping', 'Stamina', 'Strength',\n",
    "    'Long shots','Aggression', 'Interceptions', 'Att. Position', 'Vision', 'Penalties', 'Composure',\n",
    "    'Defensive awareness', 'Standing tackle', 'Sliding tackle', 'GK Diving', 'GK Handling', 'GK Kicking',\n",
    "    'GK Positioning','GK Reflexes']\n",
    "\n",
    "PLAYER_DETAIL_COLUMNS = ['PlayerId', 'PlayerName', 'Nationality', 'Age', 'Height', 'Weigh', 'Value', 'Wage'] + PLAYER_QUALITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_concatenate(a):\n",
    "    return list(np.concatenate(a))\n",
    "\n",
    "def scrap_page(page_link):\n",
    "    \"\"\"\n",
    "    Fetches the content of a webpage and returns its BeautifulSoup parser object.\n",
    "    \n",
    "    Parameters:\n",
    "    - page_link (str): The URL of the page to scrape.\n",
    "    \n",
    "    Returns:\n",
    "    - BeautifulSoup object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(page_link, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_teams_from_league(url_prefix, league_name, league_code):\n",
    "    \"\"\"\n",
    "    Extracts teams from a given league by scraping the league's webpage.\n",
    "    \n",
    "    Parameters:\n",
    "    - url_prefix (str): The prefix URL to append the league code to.\n",
    "    - league_name (str): The name of the league.\n",
    "    - league_code (str): The specific code of the league to append to the URL prefix.\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples containing league name, team name, and team link if successful, or an empty list on failure.\n",
    "    \"\"\"\n",
    "    link = f\"{url_prefix}{league_code}\"\n",
    "    page = scrap_page(link)\n",
    "\n",
    "    if page:\n",
    "        all_links = page.find_all('a', href=True)\n",
    "        team_links_inclusive = [link for link in all_links if \"/team/\" in link['href']]\n",
    "        team_info_inclusive = [(league_name, link.text.strip(), link['href']) for link in team_links_inclusive if link.text.strip()]\n",
    "        return team_info_inclusive\n",
    "    else:\n",
    "        print(\"Failed to scrape the page or parse team data.\")\n",
    "        return [(None, None, None)]\n",
    "\n",
    "def extract_players_from_team(url_prefixe, team_name, team_id, team_suffixe):\n",
    "    link = f\"{url_prefixe}{team_suffixe}\"\n",
    "    page = scrap_page(link)\n",
    "\n",
    "    if not page:\n",
    "        return [(team_name, team_id, None)]\n",
    "\n",
    "    # Locate the boundaries\n",
    "    start_marker = page.find('h5', string='Squad')\n",
    "    end_marker = page.find('h5', string='On loan')\n",
    "\n",
    "    # Extract content between the boundaries\n",
    "    squad_content = str(page).split(str(start_marker), 1)[-1].split(str(end_marker), 1)[0]\n",
    "\n",
    "    # Parse the extracted squad content to apply the provided code snippet\n",
    "    squad_soup = BeautifulSoup(squad_content, 'html.parser')\n",
    "\n",
    "    # Find all 'a' tags potentially containing team links, without applying a specific regex filter initially\n",
    "    all_links = squad_soup.find_all('a', href=True)\n",
    "\n",
    "    # Define a more inclusive criterion for filtering team links, focusing on common attributes\n",
    "    # Here, we assume team links have a specific pattern like \"/team/\" followed by numbers and the team name\n",
    "    team_links_inclusive = [link for link in all_links if \"/player/\" in link['href']]\n",
    "\n",
    "    team_info_inclusive = [(team_name, team_id, link['href']) for link in team_links_inclusive if link.text.strip()]\n",
    "\n",
    "    team_info_inclusive = list(dict.fromkeys(team_info_inclusive))\n",
    "\n",
    "    return team_info_inclusive\n",
    "\n",
    "def get_player_identity(page):\n",
    "    # Assuming the \"profile clearfix\" div is correctly provided in the html_snippet\n",
    "    # Re-parse the HTML snippet focusing on <div class=\"profile clearfix\">\n",
    "    profile_soup = page.find('div', class_=\"profile clearfix\")\n",
    "\n",
    "    # Player's name is directly within an <h1> tag\n",
    "    player_name_corrected = profile_soup.find('h1').text\n",
    "\n",
    "    # Nationality is in the title attribute of the link within <p>\n",
    "    nationality_corrected = profile_soup.find('a', title=True)['title']\n",
    "\n",
    "    # Extracting player info text again, assuming we might need to refine the approach\n",
    "    player_info_text_corrected = profile_soup.find('p').text\n",
    "\n",
    "    # Corrected regular expressions for age, height, and weight\n",
    "    # Adjusting regex to correctly match the patterns\n",
    "    age_corrected = re.search(r'(\\d+)y\\.o\\.', player_info_text_corrected)\n",
    "    height_corrected = re.search(r'(\\d+)cm', player_info_text_corrected)\n",
    "    weight_corrected = re.search(r'(\\d+)kg', player_info_text_corrected)\n",
    "\n",
    "    # Extracting matched groups if found, else None\n",
    "    age_extracted = age_corrected.group(1) if age_corrected else None\n",
    "    height_extracted = height_corrected.group(1) if height_corrected else None\n",
    "    weight_extracted = weight_corrected.group(1) if weight_corrected else None\n",
    "\n",
    "    return [player_name_corrected, nationality_corrected, age_extracted, height_extracted, weight_extracted]\n",
    "\n",
    "def get_player_postion_preferred_foot(page):\n",
    "    # Extract the value of 'Preferred foot'\n",
    "    preferred_foot_label = page.find('label', string=\"Preferred foot\")\n",
    "    preferred_foot = preferred_foot_label.next_sibling.strip() if preferred_foot_label else \"Not Found\"\n",
    "\n",
    "    # Extract the value of 'Position'\n",
    "    position_label = page.find('label', string=\"Position\")\n",
    "    position_value = position_label.find_next_sibling('span').text if position_label else \"Not Found\"\n",
    "\n",
    "    return [preferred_foot, position_value]\n",
    "\n",
    "def get_player_wage_value(page):\n",
    "    # Use regular expressions to find the divs that contain 'Value' and 'Wage' and then extract the em text\n",
    "    value_div = page.find('div', string=re.compile('Value')).find_previous_sibling('em').string\n",
    "    wage_div = page.find('div', string=re.compile('Wage')).find_previous_sibling('em').string\n",
    "\n",
    "    return [value_div, wage_div]\n",
    "\n",
    "def get_player_qualities(page, player_qualities):\n",
    "    players_quality_values = []\n",
    "\n",
    "    for quality in player_qualities:\n",
    "        v = page.find('span', string=re.compile(quality)).find_previous('em').text\n",
    "        players_quality_values.append((v))\n",
    "    \n",
    "    return players_quality_values\n",
    "\n",
    "def get_player_information(url_prefix, player_suffix, player_id, player_qualities):   \n",
    "   \n",
    "    link = f\"{url_prefix}{player_suffix}\"\n",
    "    page = scrap_page(link)\n",
    "\n",
    "    if not page:\n",
    "        return [player_id] + ['Null' for i in range(41)]\n",
    "\n",
    "    player_identity = get_player_identity(page)\n",
    "    player_position_foot = get_player_postion_preferred_foot(page)\n",
    "    player_value_wage = get_player_wage_value(page)\n",
    "    players_quality_values = get_player_qualities(player_qualities)\n",
    "\n",
    "    player_info = [player_id] + player_identity + player_position_foot + player_value_wage + players_quality_values\n",
    "\n",
    "    return player_info\n",
    "\n",
    "def get_player_link_by_batch(temp_team_df):\n",
    "    player_df = pl.DataFrame(schema=['Team', 'TeamId', 'Link'])\n",
    "    while temp_team_df.shape[0] > 0:\n",
    "        # Correct iteration over DataFrame rows using itertuples()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(extract_players_from_team, TEAM_PREFIX, row['Team'], row['TeamId'], row['Link']) for row in temp_team_df.iter_rows(named=True)]\n",
    "            list_players = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "        list_players_extended = numpy_concatenate(list_players)\n",
    "        temp_players_df = pl.DataFrame(list_players_extended, orient='row', schema={'Team':pl.String, 'TeamId':pl.String, 'Link':pl.String})\n",
    "\n",
    "        failed_requested_teams = temp_players_df \\\n",
    "            .filter(pl.col('Link').is_null()) \\\n",
    "            .select(pl.col('TeamId')) \\\n",
    "            .with_columns(pl.col(\"TeamId\").cast(pl.Int32))\n",
    "\n",
    "        temp_team_df = temp_team_df.filter(pl.col('TeamId').is_in(failed_requested_teams['TeamId']))\n",
    "\n",
    "        temp_players_df = temp_players_df \\\n",
    "            .filter(~pl.col(\"Link\").is_null()) \\\n",
    "            .with_columns(pl.col(\"Team\").cast(pl.String)) \\\n",
    "            .with_columns(pl.col(\"TeamId\").cast(pl.String)) \\\n",
    "            .with_columns(pl.col(\"Link\").cast(pl.String))\n",
    "        \n",
    "        \n",
    "        player_df = pl.concat([player_df, temp_players_df], how=\"vertical_relaxed\")\n",
    "\n",
    "        print(f\"Number of players extracted : {player_df.shape[0]}\")\n",
    "        print(f\"Number of failed extraction : {failed_requested_teams.shape[0]}\")\n",
    "\n",
    "        if failed_requested_teams.shape[0] > 0:\n",
    "            print(\"We stop requesting the website for 30 seconds in order to stop overloading their server\")\n",
    "            time.sleep(30)  # Pauses the program for 30 seconds\n",
    "\n",
    "    return player_df\n",
    "\n",
    "def get_player_details_by_batch(team_prefix, temp_player_df, player_columns, player_qualities):\n",
    "    player_details_df = pl.DataFrame(schema=player_columns)\n",
    "    while temp_player_df.shape[0] > 0:\n",
    "        # Correct iteration over DataFrame rows using itertuples()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(get_player_information, team_prefix, row['Link'], row['PlayerId'], player_qualities) for row in temp_player_df.iter_rows(named=True)]\n",
    "            list_players = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "        temp_players_detail_df = pl.DataFrame(list_players, orient='row', schema=player_columns)\n",
    "\n",
    "        failed_requested_players = temp_players_detail_df \\\n",
    "            .filter(pl.col('PlayerName') == 'Null') \\\n",
    "            .select(pl.col('PlayerId')) \\\n",
    "            .with_columns(pl.col('PlayerId').cast(pl.Int32))\n",
    "\n",
    "        temp_player_df = temp_player_df.filter(pl.col('PlayerId').is_in(failed_requested_players['PlayerId']))\n",
    "\n",
    "        temp_players_detail_df = temp_players_detail_df.filter(~pl.col(\"PlayerName\").is_null())\n",
    "\n",
    "        player_details_df = pl.concat([player_details_df, temp_players_detail_df], how=\"vertical_relaxed\")\n",
    "\n",
    "        print(f\"Number of players details extracted : {temp_players_detail_df.shape[0]}\")\n",
    "        print(f\"Number of failed extraction : {failed_requested_players.shape[0]}\")\n",
    "\n",
    "        if failed_requested_players.shape[0] > 0:\n",
    "            print(\"We stop requesting the website for 30 seconds in order to stop overloading their server\")\n",
    "            time.sleep(30)  # Pauses the program for 30 seconds\n",
    "\n",
    "    return player_details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Team Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ThreadPoolExecutor to parallelize the process\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(extract_teams_from_league, LEAGUE_PREFIX, key, value) for key, value in LEAGUE_NUMBERS.items()]\n",
    "    list_teams = [future.result() for future in concurrent.futures.as_completed(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_teams_extended = numpy_concatenate(list_teams)\n",
    "team_df = pl.DataFrame(list_teams_extended, orient='row', schema=['League', 'Team', 'Link'])\n",
    "\n",
    "team_df = team_df.with_columns(\n",
    "    pl.Series(\"TeamId\", list(range(1, len(team_df) + 1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Player Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "start = i\n",
    "factor = 50\n",
    "end = factor\n",
    "final_end = team_df.shape[0] # 272\n",
    "\n",
    "player_df = pl.DataFrame(schema=['Team', 'TeamId', 'Link'])\n",
    "while start < final_end:\n",
    "    print(f\"Start : {start}, end : {end}.\")\n",
    "    part_player_df = get_player_link_by_batch(team_df[start:end])\n",
    "    player_df = pl.concat([player_df, part_player_df], how=\"vertical_relaxed\")\n",
    "    print(f\"Total number of players: {player_df.shape[0]}\\n\")\n",
    "\n",
    "    i = i + 1\n",
    "    start = start + factor\n",
    "    end = start + factor\n",
    "\n",
    "player_df = player_df \\\n",
    "    .with_columns(pl.Series('PlayerId', list(range(1, len(player_df) + 1)))) \\\n",
    "    .with_columns(pl.col(\"TeamId\").cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n"
     ]
    }
   ],
   "source": [
    "print(len(player_df[\"Team\"].unique())) # should be 272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Player Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "start = i\n",
    "factor = 50\n",
    "end = factor\n",
    "final_end = player_df.shape[0]\n",
    "\n",
    "player_detail_df = pl.DataFrame(schema=PLAYER_DETAIL_COLUMNS)\n",
    "while start < final_end:\n",
    "    print(f\"Start : {start}, end : {end}.\")\n",
    "    part_detail_player_df = get_player_details_by_batch(TEAM_PREFIX, player_df[start:end], PLAYER_DETAIL_COLUMNS, PLAYER_QUALITIES)\n",
    "    player_detail_df = pl.concat([player_detail_df, part_detail_player_df], how=\"vertical_relaxed\")\n",
    "    player_detail_df = player_detail_df.unique(subset=['PlayerId'])\n",
    "    print(f\"Total number of players: {player_detail_df.shape[0]}\\n\")\n",
    "\n",
    "    i = i + 1\n",
    "    start = start + factor\n",
    "    end = start + factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_information(url_prefix, player_suffix, player_id, player_qualities):   \n",
    "   \n",
    "    link = f\"{url_prefix}{player_suffix}\"\n",
    "    page = scrap_page(link)\n",
    "\n",
    "    if not page:\n",
    "        return [player_id] + ['Null' for i in range(41)]\n",
    "    \n",
    "    return page\n",
    "    \"\"\"\n",
    "    # Assuming the \"profile clearfix\" div is correctly provided in the html_snippet\n",
    "    # Re-parse the HTML snippet focusing on <div class=\"profile clearfix\">\n",
    "    profile_soup = page.find('div', class_=\"profile clearfix\")\n",
    "\n",
    "    # Player's name is directly within an <h1> tag\n",
    "    player_name_corrected = profile_soup.find('h1').text\n",
    "\n",
    "    # Nationality is in the title attribute of the link within <p>\n",
    "    nationality_corrected = profile_soup.find('a', title=True)['title']\n",
    "\n",
    "    # Extracting player info text again, assuming we might need to refine the approach\n",
    "    player_info_text_corrected = profile_soup.find('p').text\n",
    "\n",
    "    # Corrected regular expressions for age, height, and weight\n",
    "    # Adjusting regex to correctly match the patterns\n",
    "    age_corrected = re.search(r'(\\d+)y\\.o\\.', player_info_text_corrected)\n",
    "    height_corrected = re.search(r'(\\d+)cm', player_info_text_corrected)\n",
    "    weight_corrected = re.search(r'(\\d+)kg', player_info_text_corrected)\n",
    "\n",
    "    # Extracting matched groups if found, else None\n",
    "    age_extracted = age_corrected.group(1) if age_corrected else None\n",
    "    height_extracted = height_corrected.group(1) if height_corrected else None\n",
    "    weight_extracted = weight_corrected.group(1) if weight_corrected else None\n",
    "\n",
    "    player_identity = [player_name_corrected, nationality_corrected, age_extracted, height_extracted, weight_extracted]\n",
    "\n",
    "    # Use regular expressions to find the divs that contain 'Value' and 'Wage' and then extract the em text\n",
    "    value_div = page.find('div', string=re.compile('Value')).find_previous_sibling('em').string\n",
    "    wage_div = page.find('div', string=re.compile('Wage')).find_previous_sibling('em').string\n",
    "\n",
    "    player_value_wage = [value_div, wage_div]\n",
    "\n",
    "    players_quality_values = []\n",
    "\n",
    "    for quality in player_qualities:\n",
    "        v = page.find('span', string=re.compile(quality)).find_previous('em').text\n",
    "        players_quality_values.append((v))\n",
    "\n",
    "    player_info = [player_id] + player_identity + player_value_wage + players_quality_values\n",
    "\n",
    "    return player_info\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_player_information(TEAM_PREFIX, '/player/188377/kyle-walker/240028/', '5', PLAYER_QUALITIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
